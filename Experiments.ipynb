{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15885abc-ac41-4612-af56-6325e1bf0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "import lpips  # pip install lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9ab973-c6f7-4497-a13e-8aed502f6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpips_distance(loss_fn, tensor_img0, tensor_img1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tensor_img0 = tensor_img0.to(device)\n",
    "    tensor_img1 = tensor_img1.to(device)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    \n",
    "    tensor_img0 = tensor_img0.unsqueeze(0) * 2 - 1\n",
    "    tensor_img1 = tensor_img1.unsqueeze(0) * 2 - 1\n",
    "    \n",
    "    # Compute distance\n",
    "    with torch.no_grad():\n",
    "        dist01 = loss_fn.forward(tensor_img0, tensor_img1)\n",
    "    \n",
    "    return dist01.item() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68b0c3e-a17b-4bc2-9efc-11c50b5640a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(data_dir, train_size=20000, test_size=2000):\n",
    "    train_dir = os.path.join(data_dir, \"train\")\n",
    "    testor rather cmd2 && cmd1 to run cmd1 only if cmd2 succeeds), and you'll need to tell nohup to start a shell_dir = os.path.join(data_dir, \"test\")\n",
    "    # Basic transform: resize/crop images and convert to tensor.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()  # Produces tensors in [0, 1]\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # If the dataset has more images than needed, take a random subset.\n",
    "    if len(train_dataset) > train_size:\n",
    "        indices = list(range(len(train_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        train_dataset = Subset(train_dataset, indices[:train_size])\n",
    "    if len(test_dataset) > test_size:\n",
    "        indices = list(range(len(test_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        test_dataset = Subset(test_dataset, indices[:test_size])\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Helper Dataset to hold a list of image samples\n",
    "class CustomListDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        data_list: list of (image_tensor, label) tuples\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6cf520-c5bd-40a4-a50f-f0aa4515a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_arch=\"alexnet\", num_classes=10):\n",
    "    if model_arch.lower() == \"alexnet\":\n",
    "        #model = models.alexnet(pretrained=True)\n",
    "        model = models.alexnet(weights=None)\n",
    "        # Replace the classifier last layer.\n",
    "        model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    else:\n",
    "        #model = models.resnet50(pretrained=True)\n",
    "        model = models.resnet50(weights=None)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_model(model, train_loader, test_loader, device, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    final_accuracy = 0\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        final_accuracy = test_model(model, test_loader, device)\n",
    "    return final_accuracy\n",
    "        \n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    return (100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8cd13c4-18d8-4274-b704-21ffefc14dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_one_image(image_tensor):\n",
    "\n",
    "    # Rearrange image dimensions: from [C, H, W] to [H, W, C] for plt.imshow().\n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image.\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4de61c8-591d-4ad0-a036-5be4c0412586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformations(image_tensor):\n",
    "    _, height, width = image_tensor.shape\n",
    "    transformations = {\n",
    "        \"horizontal_flip\": transforms.RandomHorizontalFlip(p=1.0),\n",
    "        \"vertical_flip\": transforms.RandomVerticalFlip(p=1.0),\n",
    "        \"rotation\": transforms.RandomRotation(degrees=random.randint(30,300)),\n",
    "        \"color_enhancement\": transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "        # Assuming the images are at least 224x224; adjust as necessary.\n",
    "        \"random_crop\": transforms.Compose([transforms.RandomCrop((random.randint(height // 3, 3 * height // 4 ), random.randint(width // 3, 3 * width // 4))), transforms.Resize((height,width))]),\n",
    "        \"blur\": transforms.GaussianBlur(kernel_size=(7, 25),sigma=(9, 11))\n",
    "    }\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d464966b-c71f-4a99-a75e-cffe65c93ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_one_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img,_ \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t_name, transform \u001b[38;5;129;01min\u001b[39;00m get_transformations(img)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 6\u001b[0m         \u001b[43mshow_one_image\u001b[49m(transform(img))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show_one_image' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = \"./cifar_images_train_test/CIFAR-10-images/\"\n",
    "train_dataset, test_dataset = get_datasets(data_dir)\n",
    "\n",
    "for img,_ in test_dataset:\n",
    "    for t_name, transform in get_transformations(img).items():\n",
    "        show_one_image(transform(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f7be05-127d-40a0-bf58-4422b1ed3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Clean Training (no leakage)\n",
    "def experiment1(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 1] Clean Training: No train/test leakage.\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = get_model(model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "\n",
    "\n",
    "# Experiment 2: Direct Test Leakage â€“ add every test image to train set (remove an equal number from train)\n",
    "def experiment2(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 2] Direct Test Leakage: Inserting test images into training set.\")\n",
    "    num_to_remove = len(test_dataset)\n",
    "\n",
    "    # Remove extra training images to free up space\n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(all_indices)\n",
    "    kept_indices = all_indices[num_to_remove:]\n",
    "    new_train_examples = [train_dataset[i] for i in kept_indices]\n",
    "\n",
    "    # Append all test images into training set.\n",
    "    for sample in test_dataset:\n",
    "        new_train_examples.append(sample)\n",
    "\n",
    "    new_train_dataset = CustomListDataset(new_train_examples)\n",
    "    train_loader = DataLoader(new_train_dataset, batch_size= batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    #num_classes = len(train_dataset.dataset.classes) if isinstance(train_dataset, Subset) else len(train_dataset.classes)\n",
    "    model = get_model( model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "\n",
    "# Experiment 2/3: Put each test image to the train set 6 times \n",
    "def experiment23(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 2/3] Direct Test Leakage: Inserting test images into training set 6 times.\")\n",
    "    num_to_remove = len(test_dataset)*6\n",
    "\n",
    "    # Remove extra training images to free up space\n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(all_indices)\n",
    "    kept_indices = all_indices[num_to_remove:]\n",
    "    new_train_examples = [train_dataset[i] for i in kept_indices]\n",
    "\n",
    "    # Append all test images into training set.\n",
    "    for sample in test_dataset:\n",
    "        for i in range(6):\n",
    "            new_train_examples.append(sample)\n",
    "\n",
    "    new_train_dataset = CustomListDataset(new_train_examples)\n",
    "    train_loader = DataLoader(new_train_dataset, batch_size= batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    #num_classes = len(train_dataset.dataset.classes) if isinstance(train_dataset, Subset) else len(train_dataset.classes)\n",
    "    model = get_model( model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Experiment 3: Augmented Transformation Leakage\n",
    "# For each test image, generate 6 transformed versions and add them to the training set.\n",
    "# The training set size is kept constant by removing a number of original training images.\n",
    "def experiment3(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 3] Augmented Transformation Leakage: Adding transformed test images.\")\n",
    "    transformed_images = []\n",
    "    # For each test image, create transformed versions\n",
    "    for img, label in tqdm(test_dataset, desc=\"Transforming Test Images\"):\n",
    "        transformations = get_transformations(img)\n",
    "        #for t_name in [\"hflip\", \"vflip\", \"random_rotation\", \"random_crop\", \"color_enhancement\",\"blur\"]:\n",
    "        #    transformed = transform_tensor_image(img,t_name)\n",
    "        for t_name, transform in transformations.items():\n",
    "            transformed = transform(img)\n",
    "            #show_one_image(transformed)\n",
    "            transformed_images.append((transformed, label))\n",
    "    \n",
    "    T = len(transformed_images)  # total new images from test\n",
    "    num_to_remove = T\n",
    "\n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(all_indices)\n",
    "    kept_indices = all_indices[num_to_remove:]\n",
    "    new_train_examples = [train_dataset[i] for i in kept_indices]\n",
    "\n",
    "    # Add the transformed test images.\n",
    "    new_train_examples.extend(transformed_images)\n",
    "    new_train_dataset = CustomListDataset(new_train_examples)\n",
    "    \n",
    "    train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    #num_classes = len(train_dataset.dataset.classes) if isinstance(train_dataset, Subset) else len(train_dataset.classes)\n",
    "    model = get_model(model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Experiment 4: LPIPS Filtering\n",
    "# Use LPIPS to evaluate similarities between test images and training images,\n",
    "# and remove for each test image the 6 most similar training images.\n",
    "def experiment4(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 4] LPIPS Filtering: Removing similar training images based on LPIPS.\")\n",
    "    # First, generate the training set with transformed test images, as in experiment3.\n",
    "    transformed_images = []\n",
    "    for img, label in tqdm(test_dataset, desc=\"Transforming Test Images\"):\n",
    "        transformations = get_transformations(img)\n",
    "        for t_name, transform in transformations.items():\n",
    "            transformed = transform(img)\n",
    "            transformed_images.append((transformed, label))\n",
    "    \n",
    "    T = len(transformed_images)\n",
    "    num_to_remove = T\n",
    "\n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(all_indices)\n",
    "    kept_indices = all_indices[num_to_remove:]\n",
    "    remv_indices = all_indices[:num_to_remove]\n",
    "    new_train_examples = [train_dataset[i] for i in kept_indices]\n",
    "    train_examples_removed = [train_dataset[i] for i in remv_indices] \n",
    "    \n",
    "    new_train_examples.extend(transformed_images)\n",
    "\n",
    "    # Initialization of LPIPS loss\n",
    "    loss_fn = lpips.LPIPS(net='alex')\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fn.cuda()\n",
    "    else:\n",
    "        loss_fn.cpu()\n",
    "\n",
    "    # For each test image, compute LPIPS distance to every training sample and remove the 6 most similar.\n",
    "    print(\"Filtering training images based on LPIPS distances...\")\n",
    "    indices_to_remove = set()\n",
    "    # THIS LOOP IS EXPENSIVE\n",
    "    for test_img, _ in tqdm(test_dataset, desc=\"LPIPS Filtering\"):\n",
    "        distances = []  # list of (idx, distance)\n",
    "        for j, (train_img, _) in enumerate(new_train_examples):\n",
    "            d = lpips_distance(loss_fn, train_img, test_img)\n",
    "            distances.append((j, d))\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        # Remove 6 most similar training images for this test image.\n",
    "        for k in range(6):\n",
    "            indices_to_remove.add(distances[k][0])\n",
    "\n",
    "    # Remove selected training images.\n",
    "    filtered_train_examples = [ex for idx, ex in enumerate(new_train_examples) if idx not in indices_to_remove]\n",
    "\n",
    "    # To keep the training set size consistent, we need to add extra samples.\n",
    "    if len(filtered_train_examples) < len(new_train_examples):\n",
    "        num_needed = len(new_train_examples) - len(filtered_train_examples)\n",
    "        if len(filtered_train_examples) > 0:\n",
    "            #additional_examples = random.sample(filtered_train_examples, num_needed)\n",
    "            additional_examples = random.sample(train_examples_removed, num_needed)\n",
    "            filtered_train_examples.extend(additional_examples)\n",
    "    \n",
    "    new_train_dataset = CustomListDataset(filtered_train_examples)\n",
    "    train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    #num_classes = len(train_dataset.dataset.classes) if isinstance(train_dataset, Subset) else len(train_dataset.classes)\n",
    "    model = get_model(model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "\n",
    "\n",
    "# Experiment 5: Transformation Impact Analysis\n",
    "# For each transformation, add the transformed test image to training set,\n",
    "# then for each test image compute the LPIPS distance to the 3 most similar training samples.\n",
    "# The average of these distances across test images is calculated per transformation.\n",
    "\n",
    "def experiment5_helper(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate, loss_fn, t_name): \n",
    "    # Create a training set solely from test images transformed with t_name.\n",
    "    transformed_images = []\n",
    "    for img, label in test_dataset:\n",
    "        transform = get_transformations(img).get(t_name)\n",
    "        transformed_images.append((transform(img),label))\n",
    "\n",
    "    T = len(transformed_images)\n",
    "    num_to_remove = T\n",
    "\n",
    "    all_indices = list(range(len(train_dataset)))\n",
    "    random.shuffle(all_indices)\n",
    "    kept_indices = all_indices[num_to_remove:]\n",
    "    new_train_examples = [train_dataset[i] for i in kept_indices]    \n",
    "    new_train_examples.extend(transformed_images)\n",
    "    \n",
    "    distances_per_test = []\n",
    "    \n",
    "    # For each test image, compute LPIPS to every image in the current transformed training set.\n",
    "    for test_img, _ in tqdm(test_dataset, desc=f\"LPIPS for {t_name}\"):\n",
    "        dists = []\n",
    "        for train_img, _ in new_train_examples:\n",
    "            #test_tensor = test_img.unsqueeze(0) * 2 - 1\n",
    "            #train_tensor = train_img.unsqueeze(0) * 2 - 1\n",
    "            #with torch.no_grad():\n",
    "            #    d = loss_fn(test_tensor, train_tensor).item()\n",
    "            d = lpips_distance(loss_fn, train_img, test_img)\n",
    "            dists.append(d)\n",
    "        # Find the 3 most similar images\n",
    "        dists.sort()\n",
    "        top3 = dists[:3]\n",
    "        mean_dist = sum(top3) / 3\n",
    "        distances_per_test.append(mean_dist)\n",
    "    overall_mean = sum(distances_per_test) / len(distances_per_test)\n",
    "    print(f\"Transformation: {t_name}, Mean LPIPS distance: {overall_mean:.4f}\")\n",
    "\n",
    "    \n",
    "    new_train_dataset = CustomListDataset(transformed_train)\n",
    "    train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    #num_classes = len(train_dataset.dataset.classes) if isinstance(train_dataset, Subset) else len(train_dataset.classes)\n",
    "    model = get_model(model, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    accuracy = train_model(model, train_loader, test_loader, device, epochs=epochs, lr=learning_rate)\n",
    "    \n",
    "    return overall_mean, accuracy\n",
    "    \n",
    "\n",
    "def experiment5(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate):\n",
    "    print(\"[Experiment 5] Transformation Impact Analysis\")\n",
    "    transformations = get_transformations(train_dataset[0][0])\n",
    "    results = {}\n",
    "    \n",
    "    loss_fn = lpips.LPIPS(net='alex')\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fn.cuda()\n",
    "    else:\n",
    "        loss_fn.cpu()\n",
    "        \n",
    "    # For each transformation check its impact separately:\n",
    "    for t_name, transform in transformations.items():\n",
    "        print(f\"\\nProcessing transformation: {t_name}\")\n",
    "        overall_mean, accuracy = experiment5_helper(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate, loss_fn, t_name)\n",
    "        results[t_name] = (overall_mean, accuracy)\n",
    "       \n",
    "    print(\"\\nOverall Transformation Impact Results:\")\n",
    "    for t_name, (score,accuracy) in results.items():\n",
    "        print(f\"{t_name}: mean={score:.4f} accuracy={accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7afe47-cb3a-4062-ba58-bbbbf21b4e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Experiment 5] Transformation Impact Analysis\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/msimoni/.local/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "\n",
      "Processing transformation: horizontal_flip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS for horizontal_flip:   0%|             | 9/2000 [00:54<3:21:35,  6.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m experiment \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     22\u001b[0m         experiment5(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     experiment4(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m experiment \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mexperiment5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 233\u001b[0m, in \u001b[0;36mexperiment5\u001b[0;34m(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_name, transform \u001b[38;5;129;01min\u001b[39;00m transformations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing transformation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m     overall_mean, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment5_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     results[t_name] \u001b[38;5;241m=\u001b[39m (overall_mean, accuracy)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOverall Transformation Impact Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 196\u001b[0m, in \u001b[0;36mexperiment5_helper\u001b[0;34m(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate, loss_fn, t_name)\u001b[0m\n\u001b[1;32m    190\u001b[0m dists \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_img, _ \u001b[38;5;129;01min\u001b[39;00m transformed_train:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m#test_tensor = test_img.unsqueeze(0) * 2 - 1\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m#train_tensor = train_img.unsqueeze(0) * 2 - 1\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m#    d = loss_fn(test_tensor, train_tensor).item()\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mlpips_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     dists\u001b[38;5;241m.\u001b[39mappend(d)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Find the 3 most similar images\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mlpips_distance\u001b[0;34m(loss_fn, tensor_img0, tensor_img1)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute distance\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     dist01 \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_img0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_img1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist01\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lpips/lpips.py:119\u001b[0m, in \u001b[0;36mLPIPS.forward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# v0.0 - original release had a bug, where input was not scaled\u001b[39;00m\n\u001b[1;32m    118\u001b[0m in0_input, in1_input \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_layer(in0), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_layer(in1)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (in0, in1)\n\u001b[0;32m--> 119\u001b[0m outs0, outs1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mforward(in0_input), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43min1_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m feats0, feats1, diffs \u001b[38;5;241m=\u001b[39m {}, {}, {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lpips/pretrained_networks.py:87\u001b[0m, in \u001b[0;36malexnet.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     85\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice3(h)\n\u001b[1;32m     86\u001b[0m h_relu3 \u001b[38;5;241m=\u001b[39m h\n\u001b[0;32m---> 87\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m h_relu4 \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m     89\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice5(h)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_dir = \"./cifar_images_train_test/CIFAR-10-images/\"\n",
    "    experiment = 5        # 1,2,3,4,5,\n",
    "    model = \"alexnet\"     # \"alexnet\" or \"resnet50\"\n",
    "    batch_size = 64\n",
    "    num_classes = 10\n",
    "    epochs =  20\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    train_dataset, test_dataset = get_datasets(data_dir)\n",
    "    if experiment == 1:\n",
    "        experiment1(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "    elif experiment == 2:\n",
    "        experiment2(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "    elif experiment == 23:\n",
    "        experiment23(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "    elif experiment == 3:\n",
    "        experiment3(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "    elif experiment == 4:\n",
    "        experiment4(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "    elif experiment == 5:\n",
    "        experiment5(train_dataset, test_dataset, model, batch_size, num_classes, epochs, learning_rate)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39915c0b-6da5-45fb-bc06-75bcd64da081",
   "metadata": {},
   "source": [
    "*********************\n",
    "DUMP\n",
    "***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac301f9-201f-462d-8eae-035011b5027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def transform_tensor_image(image_tensor, transformation, crop_size=224):\n",
    "    \"\"\"\n",
    "    Apply a series of transformations to an image tensor.\n",
    "    \n",
    "    Parameters:\n",
    "      - image_tensor (Tensor): Input image tensor of shape [C, H, W] with float values in [0,1].\n",
    "      - crop_size (int): Size for the random crop (default: 224)\n",
    "      \n",
    "    Returns:\n",
    "      - Transformed image tensor.\n",
    "    \"\"\"\n",
    "    # Horizontal flip: apply with 50% probability.\n",
    "    if tr---------------------------------------------------------------------------ansformation == \"hflip\":\n",
    "        image_tensor = F.hflip(image_tensor)\n",
    "           --version\n",
    "\n",
    "    # Vertical flip: apply with 50% probability.\n",
    "    if transformation == \"hflip\":\n",
    "        image_tensor = F.vflip(image_tensor)\n",
    "    \n",
    "    # Random rotation: choose an angle between -30 and +30 degrees.\n",
    "    if transformation == \"random_rotation\":\n",
    "        angle = random.randint(30,300)\n",
    "        image_tensor = F.rotate(image_tensor, angle)\n",
    "    \n",
    "    # Color enhancement:\n",
    "    # Adjust brightness, contrast, and saturation with random factors.\n",
    "    if transformation == \"color_enhancement\":\n",
    "        brightness_factor = random.uniform(0.7, 1.3)\n",
    "        contrast_factor = random.uniform(0.7, 1.3)\n",
    "        saturation_factor = random.uniform(0.7, 1.3)\n",
    "        image_tensor = F.adjust_brightness(image_tensor, brightness_factor)\n",
    "        image_tensor = F.adjust_contrast(image_tensor, contrast_factor)\n",
    "        image_tensor = F.adjust_saturation(image_tensor, saturation_factor)\n",
    "    \n",
    "    # Random crop: only perform if image is larger than the crop size.\n",
    "    if transformation == \"random_crop\":\n",
    "        _, H, W = image_tensor.shape\n",
    "        \n",
    "        left = random.randint(0, W // 4)\n",
    "        top = random.randint(0, H // 4)\n",
    "        right = random.randint(3 * W // 4, W)\n",
    "        bottom = random.randint(3 * H // 4, H)\n",
    "            \n",
    "        image_tensor = F.crop(image_tensor, left, top, right, bottom)\n",
    "        image_tensor = F.crop(image_tensor, 0, 0, H ,W )\n",
    "        \n",
    "    \n",
    "    # Gaussian blur: kernel size fixed to (5,5) and sigma randomly chosen.\n",
    "    if transformation == \"blur\":\n",
    "        image_tensor = F.gaussian_blur(image_tensor, kernel_size=(5, 5), sigma=random.uniform(0.1, 2.0))\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "# Example usage:\n",
    "# Assuming img_tensor is a float tensor of shape [C, H, W] in the range [0,1]\n",
    "# transformed_img = transform_tensor_image(img_tensor, crop_size=224)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "def apply_random_transformation(image_tensor):\n",
    "    # Get the original dimensions\n",
    "    _, height, width = image_tensor.shape\n",
    "\n",
    "    # Define possible transformations\n",
    "    possible_transforms = [\n",
    "        random.choice([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomVerticalFlip(p=1),\n",
    "            transforms.RandomRotation(degrees= random.randint(0, 360)),\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "            transforms.RandomCrop((random.randint(height // 2, height), random.randint(width // 2, width))),\n",
    "            transforms.GaussianBlur(kernel_size=(7, 13), sigma=(9, 11))\n",
    "        ]),\n",
    "        transforms.Resize((height,width))  # Ensures dimensions are maintained\n",
    "    ]\n",
    "\n",
    "    # Apply the transformation\n",
    "    transform = transforms.Compose(possible_transforms)\n",
    "\n",
    "    transformed_image = transform(image_tensor)\n",
    "\n",
    "    return (transformed_image,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73282188-216d-4f48-b49f-48da782cd142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a013f-dc8e-4d65-88d7-7c27788f2504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
